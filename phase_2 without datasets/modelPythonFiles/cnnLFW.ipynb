{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "62e929e9-1eaa-47c5-8f74-d2cacde913fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Batch [1/138], Loss: 0.3268\n",
      "Epoch [1/5], Batch [2/138], Loss: 1.8375\n",
      "Epoch [1/5], Batch [3/138], Loss: 0.3281\n",
      "Epoch [1/5], Batch [4/138], Loss: 1.0777\n",
      "Epoch [1/5], Batch [5/138], Loss: 0.3257\n",
      "Epoch [1/5], Batch [6/138], Loss: 0.6352\n",
      "Epoch [1/5], Batch [7/138], Loss: 0.3866\n",
      "Epoch [1/5], Batch [8/138], Loss: 0.4977\n",
      "Epoch [1/5], Batch [9/138], Loss: 0.5233\n",
      "Epoch [1/5], Batch [10/138], Loss: 0.3650\n",
      "Epoch [1/5], Batch [11/138], Loss: 0.2257\n",
      "Epoch [1/5], Batch [12/138], Loss: 0.3076\n",
      "Epoch [1/5], Batch [13/138], Loss: 0.2805\n",
      "Epoch [1/5], Batch [14/138], Loss: 0.4130\n",
      "Epoch [1/5], Batch [15/138], Loss: 0.3232\n",
      "Epoch [1/5], Batch [16/138], Loss: 0.3479\n",
      "Epoch [1/5], Batch [17/138], Loss: 0.3024\n",
      "Epoch [1/5], Batch [18/138], Loss: 0.2857\n",
      "Epoch [1/5], Batch [19/138], Loss: 0.3265\n",
      "Epoch [1/5], Batch [20/138], Loss: 0.2774\n",
      "Epoch [1/5], Batch [21/138], Loss: 0.3592\n",
      "Epoch [1/5], Batch [22/138], Loss: 0.2836\n",
      "Epoch [1/5], Batch [23/138], Loss: 0.2872\n",
      "Epoch [1/5], Batch [24/138], Loss: 0.2616\n",
      "Epoch [1/5], Batch [25/138], Loss: 0.3525\n",
      "Epoch [1/5], Batch [26/138], Loss: 0.3065\n",
      "Epoch [1/5], Batch [27/138], Loss: 0.5061\n",
      "Epoch [1/5], Batch [28/138], Loss: 0.3046\n",
      "Epoch [1/5], Batch [29/138], Loss: 0.2829\n",
      "Epoch [1/5], Batch [30/138], Loss: 0.2699\n",
      "Epoch [1/5], Batch [31/138], Loss: 0.3337\n",
      "Epoch [1/5], Batch [32/138], Loss: 0.2416\n",
      "Epoch [1/5], Batch [33/138], Loss: 0.4055\n",
      "Epoch [1/5], Batch [34/138], Loss: 0.2787\n",
      "Epoch [1/5], Batch [35/138], Loss: 0.3571\n",
      "Epoch [1/5], Batch [36/138], Loss: 0.2803\n",
      "Epoch [1/5], Batch [37/138], Loss: 0.2510\n",
      "Epoch [1/5], Batch [38/138], Loss: 0.3102\n",
      "Epoch [1/5], Batch [39/138], Loss: 0.3418\n",
      "Epoch [1/5], Batch [40/138], Loss: 0.3080\n",
      "Epoch [1/5], Batch [41/138], Loss: 0.2956\n",
      "Epoch [1/5], Batch [42/138], Loss: 0.3059\n",
      "Epoch [1/5], Batch [43/138], Loss: 0.2845\n",
      "Epoch [1/5], Batch [44/138], Loss: 0.2505\n",
      "Epoch [1/5], Batch [45/138], Loss: 0.3492\n",
      "Epoch [1/5], Batch [46/138], Loss: 0.3568\n",
      "Epoch [1/5], Batch [47/138], Loss: 0.2996\n",
      "Epoch [1/5], Batch [48/138], Loss: 0.3152\n",
      "Epoch [1/5], Batch [49/138], Loss: 0.3252\n",
      "Epoch [1/5], Batch [50/138], Loss: 0.3190\n",
      "Epoch [1/5], Batch [51/138], Loss: 0.2513\n",
      "Epoch [1/5], Batch [52/138], Loss: 0.2565\n",
      "Epoch [1/5], Batch [53/138], Loss: 0.2953\n",
      "Epoch [1/5], Batch [54/138], Loss: 0.2931\n",
      "Epoch [1/5], Batch [55/138], Loss: 0.2305\n",
      "Epoch [1/5], Batch [56/138], Loss: 0.3469\n",
      "Epoch [1/5], Batch [57/138], Loss: 0.3413\n",
      "Epoch [1/5], Batch [58/138], Loss: 0.2874\n",
      "Epoch [1/5], Batch [59/138], Loss: 0.3324\n",
      "Epoch [1/5], Batch [60/138], Loss: 0.3007\n",
      "Epoch [1/5], Batch [61/138], Loss: 0.2781\n",
      "Epoch [1/5], Batch [62/138], Loss: 0.2857\n",
      "Epoch [1/5], Batch [63/138], Loss: 0.2675\n",
      "Epoch [1/5], Batch [64/138], Loss: 0.2304\n",
      "Epoch [1/5], Batch [65/138], Loss: 0.2102\n",
      "Epoch [1/5], Batch [66/138], Loss: 0.3592\n",
      "Epoch [1/5], Batch [67/138], Loss: 0.3459\n",
      "Epoch [1/5], Batch [68/138], Loss: 0.3661\n",
      "Epoch [1/5], Batch [69/138], Loss: 0.4098\n",
      "Epoch [1/5], Batch [70/138], Loss: 0.3541\n",
      "Epoch [1/5], Batch [71/138], Loss: 0.2474\n",
      "Epoch [1/5], Batch [72/138], Loss: 0.3758\n",
      "Epoch [1/5], Batch [73/138], Loss: 0.3526\n",
      "Epoch [1/5], Batch [74/138], Loss: 0.2493\n",
      "Epoch [1/5], Batch [75/138], Loss: 0.2354\n",
      "Epoch [1/5], Batch [76/138], Loss: 0.3774\n",
      "Epoch [1/5], Batch [77/138], Loss: 0.4618\n",
      "Epoch [1/5], Batch [78/138], Loss: 0.2243\n",
      "Epoch [1/5], Batch [79/138], Loss: 0.2800\n",
      "Epoch [1/5], Batch [80/138], Loss: 0.2470\n",
      "Epoch [1/5], Batch [81/138], Loss: 0.2597\n",
      "Epoch [1/5], Batch [82/138], Loss: 0.2948\n",
      "Epoch [1/5], Batch [83/138], Loss: 0.3058\n",
      "Epoch [1/5], Batch [84/138], Loss: 0.2842\n",
      "Epoch [1/5], Batch [85/138], Loss: 0.2579\n",
      "Epoch [1/5], Batch [86/138], Loss: 0.3849\n",
      "Epoch [1/5], Batch [87/138], Loss: 0.2336\n",
      "Epoch [1/5], Batch [88/138], Loss: 0.2709\n",
      "Epoch [1/5], Batch [89/138], Loss: 0.2847\n",
      "Epoch [1/5], Batch [90/138], Loss: 0.2798\n",
      "Epoch [1/5], Batch [91/138], Loss: 0.3292\n",
      "Epoch [1/5], Batch [92/138], Loss: 0.2651\n",
      "Epoch [1/5], Batch [93/138], Loss: 0.2668\n",
      "Epoch [1/5], Batch [94/138], Loss: 0.2874\n",
      "Epoch [1/5], Batch [95/138], Loss: 0.2948\n",
      "Epoch [1/5], Batch [96/138], Loss: 0.2872\n",
      "Epoch [1/5], Batch [97/138], Loss: 0.3218\n",
      "Epoch [1/5], Batch [98/138], Loss: 0.2759\n",
      "Epoch [1/5], Batch [99/138], Loss: 0.3217\n",
      "Epoch [1/5], Batch [100/138], Loss: 0.3945\n",
      "Epoch [1/5], Batch [101/138], Loss: 0.2623\n",
      "Epoch [1/5], Batch [102/138], Loss: 0.2694\n",
      "Epoch [1/5], Batch [103/138], Loss: 0.2493\n",
      "Epoch [1/5], Batch [104/138], Loss: 0.2700\n",
      "Epoch [1/5], Batch [105/138], Loss: 0.3147\n",
      "Epoch [1/5], Batch [106/138], Loss: 0.2628\n",
      "Epoch [1/5], Batch [107/138], Loss: 0.3173\n",
      "Epoch [1/5], Batch [108/138], Loss: 0.3069\n",
      "Epoch [1/5], Batch [109/138], Loss: 0.2881\n",
      "Epoch [1/5], Batch [110/138], Loss: 0.2795\n",
      "Epoch [1/5], Batch [111/138], Loss: 0.2633\n",
      "Epoch [1/5], Batch [112/138], Loss: 0.3450\n",
      "Epoch [1/5], Batch [113/138], Loss: 0.2864\n",
      "Epoch [1/5], Batch [114/138], Loss: 0.2753\n",
      "Epoch [1/5], Batch [115/138], Loss: 0.3067\n",
      "Epoch [1/5], Batch [116/138], Loss: 0.3095\n",
      "Epoch [1/5], Batch [117/138], Loss: 0.2567\n",
      "Epoch [1/5], Batch [118/138], Loss: 0.3440\n",
      "Epoch [1/5], Batch [119/138], Loss: 0.3276\n",
      "Epoch [1/5], Batch [120/138], Loss: 0.3097\n",
      "Epoch [1/5], Batch [121/138], Loss: 0.2666\n",
      "Epoch [1/5], Batch [122/138], Loss: 0.3087\n",
      "Epoch [1/5], Batch [123/138], Loss: 0.2565\n",
      "Epoch [1/5], Batch [124/138], Loss: 0.2981\n",
      "Epoch [1/5], Batch [125/138], Loss: 0.2812\n",
      "Epoch [1/5], Batch [126/138], Loss: 0.3106\n",
      "Epoch [1/5], Batch [127/138], Loss: 0.2841\n",
      "Epoch [1/5], Batch [128/138], Loss: 0.3260\n",
      "Epoch [1/5], Batch [129/138], Loss: 0.2680\n",
      "Epoch [1/5], Batch [130/138], Loss: 0.3452\n",
      "Epoch [1/5], Batch [131/138], Loss: 0.2998\n",
      "Epoch [1/5], Batch [132/138], Loss: 0.2887\n",
      "Epoch [1/5], Batch [133/138], Loss: 0.3430\n",
      "Epoch [1/5], Batch [134/138], Loss: 0.2748\n",
      "Epoch [1/5], Batch [135/138], Loss: 0.3108\n",
      "Epoch [1/5], Batch [136/138], Loss: 0.3548\n",
      "Epoch [1/5], Batch [137/138], Loss: 0.1985\n",
      "Epoch [1/5], Batch [138/138], Loss: 0.2460\n",
      "Epoch [1/5] completed. Average Loss: 0.3255\n",
      "\n",
      "Epoch [2/5], Batch [1/138], Loss: 0.3162\n",
      "Epoch [2/5], Batch [2/138], Loss: 0.3089\n",
      "Epoch [2/5], Batch [3/138], Loss: 0.2095\n",
      "Epoch [2/5], Batch [4/138], Loss: 0.2561\n",
      "Epoch [2/5], Batch [5/138], Loss: 0.3916\n",
      "Epoch [2/5], Batch [6/138], Loss: 0.2655\n",
      "Epoch [2/5], Batch [7/138], Loss: 0.3111\n",
      "Epoch [2/5], Batch [8/138], Loss: 0.2967\n",
      "Epoch [2/5], Batch [9/138], Loss: 0.2702\n",
      "Epoch [2/5], Batch [10/138], Loss: 0.2777\n",
      "Epoch [2/5], Batch [11/138], Loss: 0.2678\n",
      "Epoch [2/5], Batch [12/138], Loss: 0.2815\n",
      "Epoch [2/5], Batch [13/138], Loss: 0.3183\n",
      "Epoch [2/5], Batch [14/138], Loss: 0.2710\n",
      "Epoch [2/5], Batch [15/138], Loss: 0.2073\n",
      "Epoch [2/5], Batch [16/138], Loss: 0.3597\n",
      "Epoch [2/5], Batch [17/138], Loss: 0.3368\n",
      "Epoch [2/5], Batch [18/138], Loss: 0.3741\n",
      "Epoch [2/5], Batch [19/138], Loss: 0.3264\n",
      "Epoch [2/5], Batch [20/138], Loss: 0.3743\n",
      "Epoch [2/5], Batch [21/138], Loss: 0.2774\n",
      "Epoch [2/5], Batch [22/138], Loss: 0.3219\n",
      "Epoch [2/5], Batch [23/138], Loss: 0.2856\n",
      "Epoch [2/5], Batch [24/138], Loss: 0.5389\n",
      "Epoch [2/5], Batch [25/138], Loss: 0.2880\n",
      "Epoch [2/5], Batch [26/138], Loss: 0.3551\n",
      "Epoch [2/5], Batch [27/138], Loss: 0.2921\n",
      "Epoch [2/5], Batch [28/138], Loss: 0.3344\n",
      "Epoch [2/5], Batch [29/138], Loss: 0.2566\n",
      "Epoch [2/5], Batch [30/138], Loss: 0.2495\n",
      "Epoch [2/5], Batch [31/138], Loss: 0.3005\n",
      "Epoch [2/5], Batch [32/138], Loss: 0.2960\n",
      "Epoch [2/5], Batch [33/138], Loss: 0.3055\n",
      "Epoch [2/5], Batch [34/138], Loss: 0.2528\n",
      "Epoch [2/5], Batch [35/138], Loss: 0.2341\n",
      "Epoch [2/5], Batch [36/138], Loss: 0.2691\n",
      "Epoch [2/5], Batch [37/138], Loss: 0.2560\n",
      "Epoch [2/5], Batch [38/138], Loss: 0.2490\n",
      "Epoch [2/5], Batch [39/138], Loss: 0.3700\n",
      "Epoch [2/5], Batch [40/138], Loss: 0.2284\n",
      "Epoch [2/5], Batch [41/138], Loss: 0.3148\n",
      "Epoch [2/5], Batch [42/138], Loss: 0.3847\n",
      "Epoch [2/5], Batch [43/138], Loss: 0.2681\n",
      "Epoch [2/5], Batch [44/138], Loss: 0.3428\n",
      "Epoch [2/5], Batch [45/138], Loss: 0.2693\n",
      "Epoch [2/5], Batch [46/138], Loss: 0.3074\n",
      "Epoch [2/5], Batch [47/138], Loss: 0.2608\n",
      "Epoch [2/5], Batch [48/138], Loss: 0.2741\n",
      "Epoch [2/5], Batch [49/138], Loss: 0.2333\n",
      "Epoch [2/5], Batch [50/138], Loss: 0.3453\n",
      "Epoch [2/5], Batch [51/138], Loss: 0.2889\n",
      "Epoch [2/5], Batch [52/138], Loss: 0.2059\n",
      "Epoch [2/5], Batch [53/138], Loss: 0.2238\n",
      "Epoch [2/5], Batch [54/138], Loss: 0.3040\n",
      "Epoch [2/5], Batch [55/138], Loss: 0.2825\n",
      "Epoch [2/5], Batch [56/138], Loss: 0.2751\n",
      "Epoch [2/5], Batch [57/138], Loss: 0.3201\n",
      "Epoch [2/5], Batch [58/138], Loss: 0.2472\n",
      "Epoch [2/5], Batch [59/138], Loss: 0.3403\n",
      "Epoch [2/5], Batch [60/138], Loss: 0.2946\n",
      "Epoch [2/5], Batch [61/138], Loss: 0.2619\n",
      "Epoch [2/5], Batch [62/138], Loss: 0.2783\n",
      "Epoch [2/5], Batch [63/138], Loss: 0.3249\n",
      "Epoch [2/5], Batch [64/138], Loss: 0.2728\n",
      "Epoch [2/5], Batch [65/138], Loss: 0.3432\n",
      "Epoch [2/5], Batch [66/138], Loss: 0.2800\n",
      "Epoch [2/5], Batch [67/138], Loss: 0.3043\n",
      "Epoch [2/5], Batch [68/138], Loss: 0.3971\n",
      "Epoch [2/5], Batch [69/138], Loss: 0.3262\n",
      "Epoch [2/5], Batch [70/138], Loss: 0.3856\n",
      "Epoch [2/5], Batch [71/138], Loss: 0.2431\n",
      "Epoch [2/5], Batch [72/138], Loss: 0.2837\n",
      "Epoch [2/5], Batch [73/138], Loss: 0.2570\n",
      "Epoch [2/5], Batch [74/138], Loss: 0.3127\n",
      "Epoch [2/5], Batch [75/138], Loss: 0.2755\n",
      "Epoch [2/5], Batch [76/138], Loss: 0.2412\n",
      "Epoch [2/5], Batch [77/138], Loss: 0.2651\n",
      "Epoch [2/5], Batch [78/138], Loss: 0.2677\n",
      "Epoch [2/5], Batch [79/138], Loss: 0.2703\n",
      "Epoch [2/5], Batch [80/138], Loss: 0.2784\n",
      "Epoch [2/5], Batch [81/138], Loss: 0.2741\n",
      "Epoch [2/5], Batch [82/138], Loss: 0.2871\n",
      "Epoch [2/5], Batch [83/138], Loss: 0.2908\n",
      "Epoch [2/5], Batch [84/138], Loss: 0.2691\n",
      "Epoch [2/5], Batch [85/138], Loss: 0.2591\n",
      "Epoch [2/5], Batch [86/138], Loss: 0.2877\n",
      "Epoch [2/5], Batch [87/138], Loss: 0.3111\n",
      "Epoch [2/5], Batch [88/138], Loss: 0.2690\n",
      "Epoch [2/5], Batch [89/138], Loss: 0.2617\n",
      "Epoch [2/5], Batch [90/138], Loss: 0.2914\n",
      "Epoch [2/5], Batch [91/138], Loss: 0.2689\n",
      "Epoch [2/5], Batch [92/138], Loss: 0.2905\n",
      "Epoch [2/5], Batch [93/138], Loss: 0.2715\n",
      "Epoch [2/5], Batch [94/138], Loss: 0.2697\n",
      "Epoch [2/5], Batch [95/138], Loss: 0.2703\n",
      "Epoch [2/5], Batch [96/138], Loss: 0.2603\n",
      "Epoch [2/5], Batch [97/138], Loss: 0.2761\n",
      "Epoch [2/5], Batch [98/138], Loss: 0.2637\n",
      "Epoch [2/5], Batch [99/138], Loss: 0.2575\n",
      "Epoch [2/5], Batch [100/138], Loss: 0.2489\n",
      "Epoch [2/5], Batch [101/138], Loss: 0.3168\n",
      "Epoch [2/5], Batch [102/138], Loss: 0.3275\n",
      "Epoch [2/5], Batch [103/138], Loss: 0.2707\n",
      "Epoch [2/5], Batch [104/138], Loss: 0.2710\n",
      "Epoch [2/5], Batch [105/138], Loss: 0.2681\n",
      "Epoch [2/5], Batch [106/138], Loss: 0.2553\n",
      "Epoch [2/5], Batch [107/138], Loss: 0.2938\n",
      "Epoch [2/5], Batch [108/138], Loss: 0.2811\n",
      "Epoch [2/5], Batch [109/138], Loss: 0.2649\n",
      "Epoch [2/5], Batch [110/138], Loss: 0.2615\n",
      "Epoch [2/5], Batch [111/138], Loss: 0.2922\n",
      "Epoch [2/5], Batch [112/138], Loss: 0.2799\n",
      "Epoch [2/5], Batch [113/138], Loss: 0.2773\n",
      "Epoch [2/5], Batch [114/138], Loss: 0.2555\n",
      "Epoch [2/5], Batch [115/138], Loss: 0.2573\n",
      "Epoch [2/5], Batch [116/138], Loss: 0.2959\n",
      "Epoch [2/5], Batch [117/138], Loss: 0.2804\n",
      "Epoch [2/5], Batch [118/138], Loss: 0.2761\n",
      "Epoch [2/5], Batch [119/138], Loss: 0.2532\n",
      "Epoch [2/5], Batch [120/138], Loss: 0.2834\n",
      "Epoch [2/5], Batch [121/138], Loss: 0.2608\n",
      "Epoch [2/5], Batch [122/138], Loss: 0.2820\n",
      "Epoch [2/5], Batch [123/138], Loss: 0.2637\n",
      "Epoch [2/5], Batch [124/138], Loss: 0.3444\n",
      "Epoch [2/5], Batch [125/138], Loss: 0.2623\n",
      "Epoch [2/5], Batch [126/138], Loss: 0.2862\n",
      "Epoch [2/5], Batch [127/138], Loss: 0.2725\n",
      "Epoch [2/5], Batch [128/138], Loss: 0.2781\n",
      "Epoch [2/5], Batch [129/138], Loss: 0.3013\n",
      "Epoch [2/5], Batch [130/138], Loss: 0.3317\n",
      "Epoch [2/5], Batch [131/138], Loss: 0.2730\n",
      "Epoch [2/5], Batch [132/138], Loss: 0.2720\n",
      "Epoch [2/5], Batch [133/138], Loss: 0.2781\n",
      "Epoch [2/5], Batch [134/138], Loss: 0.3534\n",
      "Epoch [2/5], Batch [135/138], Loss: 0.2673\n",
      "Epoch [2/5], Batch [136/138], Loss: 0.2570\n",
      "Epoch [2/5], Batch [137/138], Loss: 0.2715\n",
      "Epoch [2/5], Batch [138/138], Loss: 0.2549\n",
      "Epoch [2/5] completed. Average Loss: 0.2883\n",
      "\n",
      "Epoch [3/5], Batch [1/138], Loss: 0.2401\n",
      "Epoch [3/5], Batch [2/138], Loss: 0.2589\n",
      "Epoch [3/5], Batch [3/138], Loss: 0.2976\n",
      "Epoch [3/5], Batch [4/138], Loss: 0.2800\n",
      "Epoch [3/5], Batch [5/138], Loss: 0.2781\n",
      "Epoch [3/5], Batch [6/138], Loss: 0.2460\n",
      "Epoch [3/5], Batch [7/138], Loss: 0.2691\n",
      "Epoch [3/5], Batch [8/138], Loss: 0.2934\n",
      "Epoch [3/5], Batch [9/138], Loss: 0.2980\n",
      "Epoch [3/5], Batch [10/138], Loss: 0.2865\n",
      "Epoch [3/5], Batch [11/138], Loss: 0.2998\n",
      "Epoch [3/5], Batch [12/138], Loss: 0.2647\n",
      "Epoch [3/5], Batch [13/138], Loss: 0.2588\n",
      "Epoch [3/5], Batch [14/138], Loss: 0.2638\n",
      "Epoch [3/5], Batch [15/138], Loss: 0.3165\n",
      "Epoch [3/5], Batch [16/138], Loss: 0.3609\n",
      "Epoch [3/5], Batch [17/138], Loss: 0.1896\n",
      "Epoch [3/5], Batch [18/138], Loss: 0.2103\n",
      "Epoch [3/5], Batch [19/138], Loss: 0.2480\n",
      "Epoch [3/5], Batch [20/138], Loss: 0.2411\n",
      "Epoch [3/5], Batch [21/138], Loss: 0.2746\n",
      "Epoch [3/5], Batch [22/138], Loss: 0.3042\n",
      "Epoch [3/5], Batch [23/138], Loss: 0.3043\n",
      "Epoch [3/5], Batch [24/138], Loss: 0.2658\n",
      "Epoch [3/5], Batch [25/138], Loss: 0.2768\n",
      "Epoch [3/5], Batch [26/138], Loss: 0.2954\n",
      "Epoch [3/5], Batch [27/138], Loss: 0.2657\n",
      "Epoch [3/5], Batch [28/138], Loss: 0.2864\n",
      "Epoch [3/5], Batch [29/138], Loss: 0.2767\n",
      "Epoch [3/5], Batch [30/138], Loss: 0.3354\n",
      "Epoch [3/5], Batch [31/138], Loss: 0.2940\n",
      "Epoch [3/5], Batch [32/138], Loss: 0.2637\n",
      "Epoch [3/5], Batch [33/138], Loss: 0.2869\n",
      "Epoch [3/5], Batch [34/138], Loss: 0.2753\n",
      "Epoch [3/5], Batch [35/138], Loss: 0.3274\n",
      "Epoch [3/5], Batch [36/138], Loss: 0.3066\n",
      "Epoch [3/5], Batch [37/138], Loss: 0.2826\n",
      "Epoch [3/5], Batch [38/138], Loss: 0.2595\n",
      "Epoch [3/5], Batch [39/138], Loss: 0.2721\n",
      "Epoch [3/5], Batch [40/138], Loss: 0.2642\n",
      "Epoch [3/5], Batch [41/138], Loss: 0.2604\n",
      "Epoch [3/5], Batch [42/138], Loss: 0.2619\n",
      "Epoch [3/5], Batch [43/138], Loss: 0.2945\n",
      "Epoch [3/5], Batch [44/138], Loss: 0.2780\n",
      "Epoch [3/5], Batch [45/138], Loss: 0.2782\n",
      "Epoch [3/5], Batch [46/138], Loss: 0.2660\n",
      "Epoch [3/5], Batch [47/138], Loss: 0.2828\n",
      "Epoch [3/5], Batch [48/138], Loss: 0.2737\n",
      "Epoch [3/5], Batch [49/138], Loss: 0.2756\n",
      "Epoch [3/5], Batch [50/138], Loss: 0.2688\n",
      "Epoch [3/5], Batch [51/138], Loss: 0.2712\n",
      "Epoch [3/5], Batch [52/138], Loss: 0.2654\n",
      "Epoch [3/5], Batch [53/138], Loss: 0.2626\n",
      "Epoch [3/5], Batch [54/138], Loss: 0.2662\n",
      "Epoch [3/5], Batch [55/138], Loss: 0.3249\n",
      "Epoch [3/5], Batch [56/138], Loss: 0.2770\n",
      "Epoch [3/5], Batch [57/138], Loss: 0.2900\n",
      "Epoch [3/5], Batch [58/138], Loss: 0.1990\n",
      "Epoch [3/5], Batch [59/138], Loss: 0.2699\n",
      "Epoch [3/5], Batch [60/138], Loss: 0.2813\n",
      "Epoch [3/5], Batch [61/138], Loss: 0.2708\n",
      "Epoch [3/5], Batch [62/138], Loss: 0.2609\n",
      "Epoch [3/5], Batch [63/138], Loss: 0.2973\n",
      "Epoch [3/5], Batch [64/138], Loss: 0.2745\n",
      "Epoch [3/5], Batch [65/138], Loss: 0.2312\n",
      "Epoch [3/5], Batch [66/138], Loss: 0.2193\n",
      "Epoch [3/5], Batch [67/138], Loss: 0.2815\n",
      "Epoch [3/5], Batch [68/138], Loss: 0.3293\n",
      "Epoch [3/5], Batch [69/138], Loss: 0.2421\n",
      "Epoch [3/5], Batch [70/138], Loss: 0.3194\n",
      "Epoch [3/5], Batch [71/138], Loss: 0.2713\n",
      "Epoch [3/5], Batch [72/138], Loss: 0.2765\n",
      "Epoch [3/5], Batch [73/138], Loss: 0.3129\n",
      "Epoch [3/5], Batch [74/138], Loss: 0.2682\n",
      "Epoch [3/5], Batch [75/138], Loss: 0.3250\n",
      "Epoch [3/5], Batch [76/138], Loss: 0.2662\n",
      "Epoch [3/5], Batch [77/138], Loss: 0.2730\n",
      "Epoch [3/5], Batch [78/138], Loss: 0.2713\n",
      "Epoch [3/5], Batch [79/138], Loss: 0.2728\n",
      "Epoch [3/5], Batch [80/138], Loss: 0.2668\n",
      "Epoch [3/5], Batch [81/138], Loss: 0.2996\n",
      "Epoch [3/5], Batch [82/138], Loss: 0.2642\n",
      "Epoch [3/5], Batch [83/138], Loss: 0.2709\n",
      "Epoch [3/5], Batch [84/138], Loss: 0.2574\n",
      "Epoch [3/5], Batch [85/138], Loss: 0.4014\n",
      "Epoch [3/5], Batch [86/138], Loss: 0.2716\n",
      "Epoch [3/5], Batch [87/138], Loss: 0.2528\n",
      "Epoch [3/5], Batch [88/138], Loss: 0.3202\n",
      "Epoch [3/5], Batch [89/138], Loss: 0.2857\n",
      "Epoch [3/5], Batch [90/138], Loss: 0.3130\n",
      "Epoch [3/5], Batch [91/138], Loss: 0.3362\n",
      "Epoch [3/5], Batch [92/138], Loss: 0.2654\n",
      "Epoch [3/5], Batch [93/138], Loss: 0.2556\n",
      "Epoch [3/5], Batch [94/138], Loss: 0.2935\n",
      "Epoch [3/5], Batch [95/138], Loss: 0.2733\n",
      "Epoch [3/5], Batch [96/138], Loss: 0.2700\n",
      "Epoch [3/5], Batch [97/138], Loss: 0.2826\n",
      "Epoch [3/5], Batch [98/138], Loss: 0.2619\n",
      "Epoch [3/5], Batch [99/138], Loss: 0.2520\n",
      "Epoch [3/5], Batch [100/138], Loss: 0.2724\n",
      "Epoch [3/5], Batch [101/138], Loss: 0.3013\n",
      "Epoch [3/5], Batch [102/138], Loss: 0.2742\n",
      "Epoch [3/5], Batch [103/138], Loss: 0.3083\n",
      "Epoch [3/5], Batch [104/138], Loss: 0.2788\n",
      "Epoch [3/5], Batch [105/138], Loss: 0.2761\n",
      "Epoch [3/5], Batch [106/138], Loss: 0.2692\n",
      "Epoch [3/5], Batch [107/138], Loss: 0.2708\n",
      "Epoch [3/5], Batch [108/138], Loss: 0.2598\n",
      "Epoch [3/5], Batch [109/138], Loss: 0.2895\n",
      "Epoch [3/5], Batch [110/138], Loss: 0.2739\n",
      "Epoch [3/5], Batch [111/138], Loss: 0.2762\n",
      "Epoch [3/5], Batch [112/138], Loss: 0.2728\n",
      "Epoch [3/5], Batch [113/138], Loss: 0.2964\n",
      "Epoch [3/5], Batch [114/138], Loss: 0.2620\n",
      "Epoch [3/5], Batch [115/138], Loss: 0.2528\n",
      "Epoch [3/5], Batch [116/138], Loss: 0.3298\n",
      "Epoch [3/5], Batch [117/138], Loss: 0.2829\n",
      "Epoch [3/5], Batch [118/138], Loss: 0.2794\n",
      "Epoch [3/5], Batch [119/138], Loss: 0.2645\n",
      "Epoch [3/5], Batch [120/138], Loss: 0.2751\n",
      "Epoch [3/5], Batch [121/138], Loss: 0.2729\n",
      "Epoch [3/5], Batch [122/138], Loss: 0.2902\n",
      "Epoch [3/5], Batch [123/138], Loss: 0.2492\n",
      "Epoch [3/5], Batch [124/138], Loss: 0.3028\n",
      "Epoch [3/5], Batch [125/138], Loss: 0.2879\n",
      "Epoch [3/5], Batch [126/138], Loss: 0.2716\n",
      "Epoch [3/5], Batch [127/138], Loss: 0.2868\n",
      "Epoch [3/5], Batch [128/138], Loss: 0.2756\n",
      "Epoch [3/5], Batch [129/138], Loss: 0.2178\n",
      "Epoch [3/5], Batch [130/138], Loss: 0.2084\n",
      "Epoch [3/5], Batch [131/138], Loss: 0.3466\n",
      "Epoch [3/5], Batch [132/138], Loss: 0.2868\n",
      "Epoch [3/5], Batch [133/138], Loss: 0.2829\n",
      "Epoch [3/5], Batch [134/138], Loss: 0.2876\n",
      "Epoch [3/5], Batch [135/138], Loss: 0.3301\n",
      "Epoch [3/5], Batch [136/138], Loss: 0.2683\n",
      "Epoch [3/5], Batch [137/138], Loss: 0.2774\n",
      "Epoch [3/5], Batch [138/138], Loss: 0.2743\n",
      "Epoch [3/5] completed. Average Loss: 0.2783\n",
      "\n",
      "Epoch [4/5], Batch [1/138], Loss: 0.2786\n",
      "Epoch [4/5], Batch [2/138], Loss: 0.2680\n",
      "Epoch [4/5], Batch [3/138], Loss: 0.2557\n",
      "Epoch [4/5], Batch [4/138], Loss: 0.3027\n",
      "Epoch [4/5], Batch [5/138], Loss: 0.3390\n",
      "Epoch [4/5], Batch [6/138], Loss: 0.2806\n",
      "Epoch [4/5], Batch [7/138], Loss: 0.3136\n",
      "Epoch [4/5], Batch [8/138], Loss: 0.2970\n",
      "Epoch [4/5], Batch [9/138], Loss: 0.2644\n",
      "Epoch [4/5], Batch [10/138], Loss: 0.2640\n",
      "Epoch [4/5], Batch [11/138], Loss: 0.2580\n",
      "Epoch [4/5], Batch [12/138], Loss: 0.2891\n",
      "Epoch [4/5], Batch [13/138], Loss: 0.2572\n",
      "Epoch [4/5], Batch [14/138], Loss: 0.2628\n",
      "Epoch [4/5], Batch [15/138], Loss: 0.3133\n",
      "Epoch [4/5], Batch [16/138], Loss: 0.2749\n",
      "Epoch [4/5], Batch [17/138], Loss: 0.2764\n",
      "Epoch [4/5], Batch [18/138], Loss: 0.2586\n",
      "Epoch [4/5], Batch [19/138], Loss: 0.2687\n",
      "Epoch [4/5], Batch [20/138], Loss: 0.2658\n",
      "Epoch [4/5], Batch [21/138], Loss: 0.2558\n",
      "Epoch [4/5], Batch [22/138], Loss: 0.2616\n",
      "Epoch [4/5], Batch [23/138], Loss: 0.2729\n",
      "Epoch [4/5], Batch [24/138], Loss: 0.2645\n",
      "Epoch [4/5], Batch [25/138], Loss: 0.2674\n",
      "Epoch [4/5], Batch [26/138], Loss: 0.2673\n",
      "Epoch [4/5], Batch [27/138], Loss: 0.2765\n",
      "Epoch [4/5], Batch [28/138], Loss: 0.2844\n",
      "Epoch [4/5], Batch [29/138], Loss: 0.2524\n",
      "Epoch [4/5], Batch [30/138], Loss: 0.2888\n",
      "Epoch [4/5], Batch [31/138], Loss: 0.2905\n",
      "Epoch [4/5], Batch [32/138], Loss: 0.2580\n",
      "Epoch [4/5], Batch [33/138], Loss: 0.2746\n",
      "Epoch [4/5], Batch [34/138], Loss: 0.2525\n",
      "Epoch [4/5], Batch [35/138], Loss: 0.2751\n",
      "Epoch [4/5], Batch [36/138], Loss: 0.2476\n",
      "Epoch [4/5], Batch [37/138], Loss: 0.2409\n",
      "Epoch [4/5], Batch [38/138], Loss: 0.3206\n",
      "Epoch [4/5], Batch [39/138], Loss: 0.2650\n",
      "Epoch [4/5], Batch [40/138], Loss: 0.2738\n",
      "Epoch [4/5], Batch [41/138], Loss: 0.2801\n",
      "Epoch [4/5], Batch [42/138], Loss: 0.2776\n",
      "Epoch [4/5], Batch [43/138], Loss: 0.2651\n",
      "Epoch [4/5], Batch [44/138], Loss: 0.2741\n",
      "Epoch [4/5], Batch [45/138], Loss: 0.2569\n",
      "Epoch [4/5], Batch [46/138], Loss: 0.2802\n",
      "Epoch [4/5], Batch [47/138], Loss: 0.3215\n",
      "Epoch [4/5], Batch [48/138], Loss: 0.2604\n",
      "Epoch [4/5], Batch [49/138], Loss: 0.2377\n",
      "Epoch [4/5], Batch [50/138], Loss: 0.3292\n",
      "Epoch [4/5], Batch [51/138], Loss: 0.2826\n",
      "Epoch [4/5], Batch [52/138], Loss: 0.2833\n",
      "Epoch [4/5], Batch [53/138], Loss: 0.2584\n",
      "Epoch [4/5], Batch [54/138], Loss: 0.2613\n",
      "Epoch [4/5], Batch [55/138], Loss: 0.2855\n",
      "Epoch [4/5], Batch [56/138], Loss: 0.2662\n",
      "Epoch [4/5], Batch [57/138], Loss: 0.3064\n",
      "Epoch [4/5], Batch [58/138], Loss: 0.2416\n",
      "Epoch [4/5], Batch [59/138], Loss: 0.2639\n",
      "Epoch [4/5], Batch [60/138], Loss: 0.2495\n",
      "Epoch [4/5], Batch [61/138], Loss: 0.2362\n",
      "Epoch [4/5], Batch [62/138], Loss: 0.2674\n",
      "Epoch [4/5], Batch [63/138], Loss: 0.3166\n",
      "Epoch [4/5], Batch [64/138], Loss: 0.2680\n",
      "Epoch [4/5], Batch [65/138], Loss: 0.2546\n",
      "Epoch [4/5], Batch [66/138], Loss: 0.3192\n",
      "Epoch [4/5], Batch [67/138], Loss: 0.2422\n",
      "Epoch [4/5], Batch [68/138], Loss: 0.2604\n",
      "Epoch [4/5], Batch [69/138], Loss: 0.2785\n",
      "Epoch [4/5], Batch [70/138], Loss: 0.2602\n",
      "Epoch [4/5], Batch [71/138], Loss: 0.2649\n",
      "Epoch [4/5], Batch [72/138], Loss: 0.2764\n",
      "Epoch [4/5], Batch [73/138], Loss: 0.2675\n",
      "Epoch [4/5], Batch [74/138], Loss: 0.2718\n",
      "Epoch [4/5], Batch [75/138], Loss: 0.2541\n",
      "Epoch [4/5], Batch [76/138], Loss: 0.2613\n",
      "Epoch [4/5], Batch [77/138], Loss: 0.2855\n",
      "Epoch [4/5], Batch [78/138], Loss: 0.2577\n",
      "Epoch [4/5], Batch [79/138], Loss: 0.2797\n",
      "Epoch [4/5], Batch [80/138], Loss: 0.2730\n",
      "Epoch [4/5], Batch [81/138], Loss: 0.2608\n",
      "Epoch [4/5], Batch [82/138], Loss: 0.2792\n",
      "Epoch [4/5], Batch [83/138], Loss: 0.2560\n",
      "Epoch [4/5], Batch [84/138], Loss: 0.2734\n",
      "Epoch [4/5], Batch [85/138], Loss: 0.2465\n",
      "Epoch [4/5], Batch [86/138], Loss: 0.2551\n",
      "Epoch [4/5], Batch [87/138], Loss: 0.2615\n",
      "Epoch [4/5], Batch [88/138], Loss: 0.2730\n",
      "Epoch [4/5], Batch [89/138], Loss: 0.2708\n",
      "Epoch [4/5], Batch [90/138], Loss: 0.2700\n",
      "Epoch [4/5], Batch [91/138], Loss: 0.2674\n",
      "Epoch [4/5], Batch [92/138], Loss: 0.2801\n",
      "Epoch [4/5], Batch [93/138], Loss: 0.2711\n",
      "Epoch [4/5], Batch [94/138], Loss: 0.2492\n",
      "Epoch [4/5], Batch [95/138], Loss: 0.2773\n",
      "Epoch [4/5], Batch [96/138], Loss: 0.3163\n",
      "Epoch [4/5], Batch [97/138], Loss: 0.2772\n",
      "Epoch [4/5], Batch [98/138], Loss: 0.2651\n",
      "Epoch [4/5], Batch [99/138], Loss: 0.2635\n",
      "Epoch [4/5], Batch [100/138], Loss: 0.2659\n",
      "Epoch [4/5], Batch [101/138], Loss: 0.2769\n",
      "Epoch [4/5], Batch [102/138], Loss: 0.2690\n",
      "Epoch [4/5], Batch [103/138], Loss: 0.2641\n",
      "Epoch [4/5], Batch [104/138], Loss: 0.2560\n",
      "Epoch [4/5], Batch [105/138], Loss: 0.2967\n",
      "Epoch [4/5], Batch [106/138], Loss: 0.2485\n",
      "Epoch [4/5], Batch [107/138], Loss: 0.2717\n",
      "Epoch [4/5], Batch [108/138], Loss: 0.2628\n",
      "Epoch [4/5], Batch [109/138], Loss: 0.2431\n",
      "Epoch [4/5], Batch [110/138], Loss: 0.2594\n",
      "Epoch [4/5], Batch [111/138], Loss: 0.2686\n",
      "Epoch [4/5], Batch [112/138], Loss: 0.2293\n",
      "Epoch [4/5], Batch [113/138], Loss: 0.2828\n",
      "Epoch [4/5], Batch [114/138], Loss: 0.2533\n",
      "Epoch [4/5], Batch [115/138], Loss: 0.2445\n",
      "Epoch [4/5], Batch [116/138], Loss: 0.2624\n",
      "Epoch [4/5], Batch [117/138], Loss: 0.2926\n",
      "Epoch [4/5], Batch [118/138], Loss: 0.2609\n",
      "Epoch [4/5], Batch [119/138], Loss: 0.2610\n",
      "Epoch [4/5], Batch [120/138], Loss: 0.2629\n",
      "Epoch [4/5], Batch [121/138], Loss: 0.2534\n",
      "Epoch [4/5], Batch [122/138], Loss: 0.2276\n",
      "Epoch [4/5], Batch [123/138], Loss: 0.2710\n",
      "Epoch [4/5], Batch [124/138], Loss: 0.2319\n",
      "Epoch [4/5], Batch [125/138], Loss: 0.2560\n",
      "Epoch [4/5], Batch [126/138], Loss: 0.2690\n",
      "Epoch [4/5], Batch [127/138], Loss: 0.3458\n",
      "Epoch [4/5], Batch [128/138], Loss: 0.2982\n",
      "Epoch [4/5], Batch [129/138], Loss: 0.2790\n",
      "Epoch [4/5], Batch [130/138], Loss: 0.2639\n",
      "Epoch [4/5], Batch [131/138], Loss: 0.2666\n",
      "Epoch [4/5], Batch [132/138], Loss: 0.2678\n",
      "Epoch [4/5], Batch [133/138], Loss: 0.2640\n",
      "Epoch [4/5], Batch [134/138], Loss: 0.2564\n",
      "Epoch [4/5], Batch [135/138], Loss: 0.2746\n",
      "Epoch [4/5], Batch [136/138], Loss: 0.2486\n",
      "Epoch [4/5], Batch [137/138], Loss: 0.2603\n",
      "Epoch [4/5], Batch [138/138], Loss: 0.2695\n",
      "Epoch [4/5] completed. Average Loss: 0.2703\n",
      "\n",
      "Epoch [5/5], Batch [1/138], Loss: 0.2644\n",
      "Epoch [5/5], Batch [2/138], Loss: 0.2598\n",
      "Epoch [5/5], Batch [3/138], Loss: 0.2839\n",
      "Epoch [5/5], Batch [4/138], Loss: 0.2463\n",
      "Epoch [5/5], Batch [5/138], Loss: 0.2812\n",
      "Epoch [5/5], Batch [6/138], Loss: 0.2834\n",
      "Epoch [5/5], Batch [7/138], Loss: 0.2822\n",
      "Epoch [5/5], Batch [8/138], Loss: 0.2510\n",
      "Epoch [5/5], Batch [9/138], Loss: 0.2707\n",
      "Epoch [5/5], Batch [10/138], Loss: 0.2560\n",
      "Epoch [5/5], Batch [11/138], Loss: 0.2588\n",
      "Epoch [5/5], Batch [12/138], Loss: 0.2678\n",
      "Epoch [5/5], Batch [13/138], Loss: 0.3019\n",
      "Epoch [5/5], Batch [14/138], Loss: 0.2782\n",
      "Epoch [5/5], Batch [15/138], Loss: 0.2927\n",
      "Epoch [5/5], Batch [16/138], Loss: 0.2463\n",
      "Epoch [5/5], Batch [17/138], Loss: 0.2929\n",
      "Epoch [5/5], Batch [18/138], Loss: 0.2857\n",
      "Epoch [5/5], Batch [19/138], Loss: 0.2652\n",
      "Epoch [5/5], Batch [20/138], Loss: 0.2694\n",
      "Epoch [5/5], Batch [21/138], Loss: 0.2854\n",
      "Epoch [5/5], Batch [22/138], Loss: 0.3468\n",
      "Epoch [5/5], Batch [23/138], Loss: 0.2694\n",
      "Epoch [5/5], Batch [24/138], Loss: 0.2829\n",
      "Epoch [5/5], Batch [25/138], Loss: 0.2606\n",
      "Epoch [5/5], Batch [26/138], Loss: 0.2560\n",
      "Epoch [5/5], Batch [27/138], Loss: 0.2587\n",
      "Epoch [5/5], Batch [28/138], Loss: 0.3167\n",
      "Epoch [5/5], Batch [29/138], Loss: 0.3432\n",
      "Epoch [5/5], Batch [30/138], Loss: 0.2740\n",
      "Epoch [5/5], Batch [31/138], Loss: 0.2421\n",
      "Epoch [5/5], Batch [32/138], Loss: 0.2893\n",
      "Epoch [5/5], Batch [33/138], Loss: 0.2858\n",
      "Epoch [5/5], Batch [34/138], Loss: 0.2733\n",
      "Epoch [5/5], Batch [35/138], Loss: 0.2717\n",
      "Epoch [5/5], Batch [36/138], Loss: 0.2728\n",
      "Epoch [5/5], Batch [37/138], Loss: 0.3068\n",
      "Epoch [5/5], Batch [38/138], Loss: 0.2522\n",
      "Epoch [5/5], Batch [39/138], Loss: 0.2702\n",
      "Epoch [5/5], Batch [40/138], Loss: 0.2765\n",
      "Epoch [5/5], Batch [41/138], Loss: 0.2780\n",
      "Epoch [5/5], Batch [42/138], Loss: 0.2698\n",
      "Epoch [5/5], Batch [43/138], Loss: 0.2571\n",
      "Epoch [5/5], Batch [44/138], Loss: 0.2629\n",
      "Epoch [5/5], Batch [45/138], Loss: 0.2716\n",
      "Epoch [5/5], Batch [46/138], Loss: 0.2648\n",
      "Epoch [5/5], Batch [47/138], Loss: 0.2545\n",
      "Epoch [5/5], Batch [48/138], Loss: 0.3002\n",
      "Epoch [5/5], Batch [49/138], Loss: 0.2682\n",
      "Epoch [5/5], Batch [50/138], Loss: 0.2633\n",
      "Epoch [5/5], Batch [51/138], Loss: 0.2537\n",
      "Epoch [5/5], Batch [52/138], Loss: 0.2651\n",
      "Epoch [5/5], Batch [53/138], Loss: 0.2705\n",
      "Epoch [5/5], Batch [54/138], Loss: 0.2687\n",
      "Epoch [5/5], Batch [55/138], Loss: 0.2526\n",
      "Epoch [5/5], Batch [56/138], Loss: 0.2686\n",
      "Epoch [5/5], Batch [57/138], Loss: 0.2816\n",
      "Epoch [5/5], Batch [58/138], Loss: 0.2599\n",
      "Epoch [5/5], Batch [59/138], Loss: 0.2710\n",
      "Epoch [5/5], Batch [60/138], Loss: 0.2541\n",
      "Epoch [5/5], Batch [61/138], Loss: 0.2573\n",
      "Epoch [5/5], Batch [62/138], Loss: 0.2601\n",
      "Epoch [5/5], Batch [63/138], Loss: 0.2622\n",
      "Epoch [5/5], Batch [64/138], Loss: 0.2690\n",
      "Epoch [5/5], Batch [65/138], Loss: 0.2661\n",
      "Epoch [5/5], Batch [66/138], Loss: 0.2545\n",
      "Epoch [5/5], Batch [67/138], Loss: 0.2698\n",
      "Epoch [5/5], Batch [68/138], Loss: 0.2717\n",
      "Epoch [5/5], Batch [69/138], Loss: 0.2600\n",
      "Epoch [5/5], Batch [70/138], Loss: 0.2799\n",
      "Epoch [5/5], Batch [71/138], Loss: 0.2592\n",
      "Epoch [5/5], Batch [72/138], Loss: 0.2631\n",
      "Epoch [5/5], Batch [73/138], Loss: 0.2702\n",
      "Epoch [5/5], Batch [74/138], Loss: 0.2605\n",
      "Epoch [5/5], Batch [75/138], Loss: 0.2647\n",
      "Epoch [5/5], Batch [76/138], Loss: 0.2815\n",
      "Epoch [5/5], Batch [77/138], Loss: 0.2732\n",
      "Epoch [5/5], Batch [78/138], Loss: 0.2590\n",
      "Epoch [5/5], Batch [79/138], Loss: 0.2698\n",
      "Epoch [5/5], Batch [80/138], Loss: 0.2812\n",
      "Epoch [5/5], Batch [81/138], Loss: 0.2489\n",
      "Epoch [5/5], Batch [82/138], Loss: 0.2527\n",
      "Epoch [5/5], Batch [83/138], Loss: 0.2582\n",
      "Epoch [5/5], Batch [84/138], Loss: 0.2414\n",
      "Epoch [5/5], Batch [85/138], Loss: 0.3069\n",
      "Epoch [5/5], Batch [86/138], Loss: 0.3700\n",
      "Epoch [5/5], Batch [87/138], Loss: 0.2838\n",
      "Epoch [5/5], Batch [88/138], Loss: 0.2257\n",
      "Epoch [5/5], Batch [89/138], Loss: 0.2195\n",
      "Epoch [5/5], Batch [90/138], Loss: 0.2417\n",
      "Epoch [5/5], Batch [91/138], Loss: 0.2568\n",
      "Epoch [5/5], Batch [92/138], Loss: 0.2641\n",
      "Epoch [5/5], Batch [93/138], Loss: 0.2378\n",
      "Epoch [5/5], Batch [94/138], Loss: 0.2966\n",
      "Epoch [5/5], Batch [95/138], Loss: 0.2907\n",
      "Epoch [5/5], Batch [96/138], Loss: 0.2837\n",
      "Epoch [5/5], Batch [97/138], Loss: 0.2885\n",
      "Epoch [5/5], Batch [98/138], Loss: 0.2623\n",
      "Epoch [5/5], Batch [99/138], Loss: 0.2800\n",
      "Epoch [5/5], Batch [100/138], Loss: 0.3218\n",
      "Epoch [5/5], Batch [101/138], Loss: 0.3838\n",
      "Epoch [5/5], Batch [102/138], Loss: 0.3093\n",
      "Epoch [5/5], Batch [103/138], Loss: 0.2572\n",
      "Epoch [5/5], Batch [104/138], Loss: 0.2648\n",
      "Epoch [5/5], Batch [105/138], Loss: 0.2601\n",
      "Epoch [5/5], Batch [106/138], Loss: 0.2723\n",
      "Epoch [5/5], Batch [107/138], Loss: 0.2667\n",
      "Epoch [5/5], Batch [108/138], Loss: 0.2667\n",
      "Epoch [5/5], Batch [109/138], Loss: 0.2862\n",
      "Epoch [5/5], Batch [110/138], Loss: 0.2581\n",
      "Epoch [5/5], Batch [111/138], Loss: 0.2499\n",
      "Epoch [5/5], Batch [112/138], Loss: 0.2603\n",
      "Epoch [5/5], Batch [113/138], Loss: 0.2812\n",
      "Epoch [5/5], Batch [114/138], Loss: 0.2685\n",
      "Epoch [5/5], Batch [115/138], Loss: 0.2573\n",
      "Epoch [5/5], Batch [116/138], Loss: 0.2742\n",
      "Epoch [5/5], Batch [117/138], Loss: 0.2774\n",
      "Epoch [5/5], Batch [118/138], Loss: 0.2581\n",
      "Epoch [5/5], Batch [119/138], Loss: 0.2777\n",
      "Epoch [5/5], Batch [120/138], Loss: 0.2712\n",
      "Epoch [5/5], Batch [121/138], Loss: 0.2664\n",
      "Epoch [5/5], Batch [122/138], Loss: 0.2660\n",
      "Epoch [5/5], Batch [123/138], Loss: 0.2519\n",
      "Epoch [5/5], Batch [124/138], Loss: 0.2833\n",
      "Epoch [5/5], Batch [125/138], Loss: 0.2381\n",
      "Epoch [5/5], Batch [126/138], Loss: 0.2705\n",
      "Epoch [5/5], Batch [127/138], Loss: 0.2576\n",
      "Epoch [5/5], Batch [128/138], Loss: 0.2560\n",
      "Epoch [5/5], Batch [129/138], Loss: 0.2608\n",
      "Epoch [5/5], Batch [130/138], Loss: 0.2488\n",
      "Epoch [5/5], Batch [131/138], Loss: 0.2528\n",
      "Epoch [5/5], Batch [132/138], Loss: 0.2626\n",
      "Epoch [5/5], Batch [133/138], Loss: 0.2648\n",
      "Epoch [5/5], Batch [134/138], Loss: 0.2542\n",
      "Epoch [5/5], Batch [135/138], Loss: 0.2605\n",
      "Epoch [5/5], Batch [136/138], Loss: 0.2771\n",
      "Epoch [5/5], Batch [137/138], Loss: 0.2657\n",
      "Epoch [5/5], Batch [138/138], Loss: 0.2381\n",
      "Epoch [5/5] completed. Average Loss: 0.2706\n",
      "\n",
      "Evaluation Batch [10/63]\n",
      "Evaluation Batch [20/63]\n",
      "Evaluation Batch [30/63]\n",
      "Evaluation Batch [40/63]\n",
      "Evaluation Batch [50/63]\n",
      "Evaluation Batch [60/63]\n",
      "Test Accuracy: 80.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define data paths\n",
    "root_dir = '/Users/dionbaldsing/Documents/CMU/AIFORMED/projectFahad/archiveLFW'\n",
    "images_dir = os.path.join(root_dir, 'lfw-deepfunneled', 'lfw-deepfunneled')\n",
    "\n",
    "# Paths to the  CSV files\n",
    "matchpairs_train_csv = os.path.join(root_dir, 'matchpairsDevTrain.csv')\n",
    "mismatchpairs_train_csv = os.path.join(root_dir, 'mismatchpairsDevTrain.csv')\n",
    "matchpairs_test_csv = os.path.join(root_dir, 'matchpairsDevTest.csv')\n",
    "mismatchpairs_test_csv = os.path.join(root_dir, 'mismatchpairsDevTest.csv')\n",
    "\n",
    "#  transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    # Add normalization if needed\n",
    "    # transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "#  Dataset class boiler code from snippets taken from kaggle.com\n",
    "class LFWDataset(Dataset):\n",
    "    def __init__(self, matches_csv, mismatches_csv, images_dir, transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Load matched pairs\n",
    "        self.matches = pd.read_csv(matches_csv)\n",
    "        self.matches['label'] = 1  # Matched pairs have label 1\n",
    "\n",
    "        # Adjusted column names for matched pairs\n",
    "        self.matches['img1_path'] = self.matches.apply(\n",
    "            lambda row: os.path.join(\n",
    "                self.images_dir,\n",
    "                row['name'].strip(),\n",
    "                f\"{row['name'].strip()}_{int(row['imagenum1']):04d}.jpg\"\n",
    "            ), axis=1)\n",
    "        self.matches['img2_path'] = self.matches.apply(\n",
    "            lambda row: os.path.join(\n",
    "                self.images_dir,\n",
    "                row['name'].strip(),\n",
    "                f\"{row['name'].strip()}_{int(row['imagenum2']):04d}.jpg\"\n",
    "            ), axis=1)\n",
    "\n",
    "        # Load mismatched pairs\n",
    "        self.mismatches = pd.read_csv(mismatches_csv)\n",
    "        self.mismatches['label'] = 0  # Mismatched pairs have label 0\n",
    "\n",
    "        # Adjusted column names for mismatched pairs\n",
    "        self.mismatches['img1_path'] = self.mismatches.apply(\n",
    "            lambda row: os.path.join(\n",
    "                self.images_dir,\n",
    "                row['name'].strip(),\n",
    "                f\"{row['name'].strip()}_{int(row['imagenum1']):04d}.jpg\"\n",
    "            ), axis=1)\n",
    "        self.mismatches['img2_path'] = self.mismatches.apply(\n",
    "            lambda row: os.path.join(\n",
    "                self.images_dir,\n",
    "                row['name.1'].strip(),\n",
    "                f\"{row['name.1'].strip()}_{int(row['imagenum2']):04d}.jpg\"\n",
    "            ), axis=1)\n",
    "\n",
    "        # Combine matches and mismatches\n",
    "        self.data = pd.concat(\n",
    "            [self.matches[['img1_path', 'img2_path', 'label']],\n",
    "             self.mismatches[['img1_path', 'img2_path', 'label']]],\n",
    "            ignore_index=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1_path = self.data.iloc[idx]['img1_path']\n",
    "        img2_path = self.data.iloc[idx]['img2_path']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Load images with error handling\n",
    "        try:\n",
    "            img1 = Image.open(img1_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img1_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        try:\n",
    "            img2 = Image.open(img2_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File not found: {img2_path}\")\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = torch.tensor([label], dtype=torch.float32)\n",
    "\n",
    "        return img1, img2, label\n",
    "\n",
    "# 4. Creating dataloaders\n",
    "train_dataset = LFWDataset(\n",
    "    matches_csv=matchpairs_train_csv,\n",
    "    mismatches_csv=mismatchpairs_train_csv,\n",
    "    images_dir=images_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = LFWDataset(\n",
    "    matches_csv=matchpairs_test_csv,\n",
    "    mismatches_csv=mismatchpairs_test_csv,\n",
    "    images_dir=images_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "#  implementing the Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=10),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Output: (batch_size, 64, 59, 59)\n",
    "            nn.Conv2d(64, 128, kernel_size=7),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Output: (batch_size, 128, 26, 26)\n",
    "            nn.Conv2d(128, 128, kernel_size=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # Output: (batch_size, 128, 11, 11)\n",
    "            nn.Conv2d(128, 256, kernel_size=4),\n",
    "            nn.ReLU(inplace=True),  # Output: (batch_size, 256, 8, 8)\n",
    "            # No MaxPool here\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 8 * 8, 4096),  # input size\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size(0), -1)  # flattenning the image\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# 6. Loss function and optimizer\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        # Using constrastive Loss function\n",
    "        loss = torch.mean(\n",
    "            (1 - label) * torch.pow(euclidean_distance, 2) +  # For matched pairs\n",
    "            label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)  # For mismatched pairs\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "\n",
    "num_epochs = 5  # Adjust as needed\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (img1, img2, label) in enumerate(train_loader):\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] completed. Average Loss: {epoch_loss:.4f}\\n\")\n",
    "    \n",
    "# 8. accuracy calculation\n",
    "def evaluate(model, dataloader, device, threshold=1.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (img1, img2, label) in enumerate(dataloader):\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "            # Predict similar if distance < threshold\n",
    "            prediction = (euclidean_distance < threshold).float()\n",
    "            correct += (prediction == label).sum().item()\n",
    "            total += label.size(0)\n",
    "\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f\"Evaluation Batch [{batch_idx+1}/{len(dataloader)}]\")\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "test_accuracy = evaluate(model, test_loader, device)\n",
    "print(f\"Test Accuracy: {test_accuracy * 10:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6b78fa03-64b3-474e-bc15-35f21d76d033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to siamese_network.pth\n"
     ]
    }
   ],
   "source": [
    "#  Saving the trained model\n",
    "model_save_path = 'siamese_network.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dcc6eb41-4cec-4223-b16d-398ca0dcbc7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SiameseNetwork:\n\tMissing key(s) in state_dict: \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\", \"cnn.4.weight\", \"cnn.4.bias\", \"cnn.5.weight\", \"cnn.5.bias\", \"cnn.5.running_mean\", \"cnn.5.running_var\", \"cnn.8.weight\", \"cnn.8.bias\", \"cnn.9.running_mean\", \"cnn.9.running_var\", \"cnn.12.weight\", \"cnn.12.bias\", \"cnn.13.weight\", \"cnn.13.bias\", \"cnn.13.running_mean\", \"cnn.13.running_var\", \"fc.1.weight\", \"fc.1.bias\", \"fc.1.running_mean\", \"fc.1.running_var\", \"fc.4.weight\", \"fc.4.bias\", \"fc.5.weight\", \"fc.5.bias\", \"fc.5.running_mean\", \"fc.5.running_var\", \"fc.8.weight\", \"fc.8.bias\", \"fc.9.weight\", \"fc.9.bias\", \"fc.9.running_mean\", \"fc.9.running_var\", \"fc.12.weight\", \"fc.12.bias\". \n\tUnexpected key(s) in state_dict: \"cnn.3.weight\", \"cnn.3.bias\", \"cnn.6.weight\", \"cnn.6.bias\". \n\tsize mismatch for cnn.9.weight: copying a param with shape torch.Size([256, 128, 4, 4]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for cnn.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc.0.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([1024, 16384]).\n\tsize mismatch for fc.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m inference_model \u001b[38;5;241m=\u001b[39m SiameseNetwork()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the saved state dictionary\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m inference_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_save_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m      5\u001b[0m inference_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 12. Testing with two images\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SiameseNetwork:\n\tMissing key(s) in state_dict: \"cnn.1.weight\", \"cnn.1.bias\", \"cnn.1.running_mean\", \"cnn.1.running_var\", \"cnn.4.weight\", \"cnn.4.bias\", \"cnn.5.weight\", \"cnn.5.bias\", \"cnn.5.running_mean\", \"cnn.5.running_var\", \"cnn.8.weight\", \"cnn.8.bias\", \"cnn.9.running_mean\", \"cnn.9.running_var\", \"cnn.12.weight\", \"cnn.12.bias\", \"cnn.13.weight\", \"cnn.13.bias\", \"cnn.13.running_mean\", \"cnn.13.running_var\", \"fc.1.weight\", \"fc.1.bias\", \"fc.1.running_mean\", \"fc.1.running_var\", \"fc.4.weight\", \"fc.4.bias\", \"fc.5.weight\", \"fc.5.bias\", \"fc.5.running_mean\", \"fc.5.running_var\", \"fc.8.weight\", \"fc.8.bias\", \"fc.9.weight\", \"fc.9.bias\", \"fc.9.running_mean\", \"fc.9.running_var\", \"fc.12.weight\", \"fc.12.bias\". \n\tUnexpected key(s) in state_dict: \"cnn.3.weight\", \"cnn.3.bias\", \"cnn.6.weight\", \"cnn.6.bias\". \n\tsize mismatch for cnn.9.weight: copying a param with shape torch.Size([256, 128, 4, 4]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for cnn.9.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for fc.0.weight: copying a param with shape torch.Size([4096, 16384]) from checkpoint, the shape in current model is torch.Size([1024, 16384]).\n\tsize mismatch for fc.0.bias: copying a param with shape torch.Size([4096]) from checkpoint, the shape in current model is torch.Size([1024])."
     ]
    }
   ],
   "source": [
    "inference_model = SiameseNetwork().to(device)\n",
    "\n",
    "# Load the saved state dictionary\n",
    "inference_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "inference_model.eval()\n",
    "\n",
    "# 12. Testing with two images\n",
    "def compare_images(model, image1_path, image2_path, transform, device, threshold=1.0):\n",
    "    # Load and preprocess images\n",
    "    img1 = Image.open(image1_path).convert('RGB')\n",
    "    img2 = Image.open(image2_path).convert('RGB')\n",
    "\n",
    "    img1 = transform(img1).unsqueeze(0).to(device)\n",
    "    img2 = transform(img2).unsqueeze(0).to(device)\n",
    "\n",
    "    # Obtain embeddings\n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(img1, img2)\n",
    "\n",
    "    # Compute Euclidean distance\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    distance = euclidean_distance.item()\n",
    "\n",
    "    # Make prediction\n",
    "    is_same_person = distance < threshold\n",
    "\n",
    "    # Output the result\n",
    "    print(f\"Euclidean Distance: {distance:.4f}\")\n",
    "    if is_same_person:\n",
    "        print(\"The images are of the **same** person.\")\n",
    "    else:\n",
    "        print(\"The images are of **different** people.\")\n",
    "\n",
    "    return distance, is_same_person\n",
    "\n",
    "# Example usage:\n",
    "# Paths to your test images\n",
    "test_image1_path = '/Users/dionbaldsing/Downloads/dion/d1.jpeg'\n",
    "test_image2_path = '/Users/dionbaldsing/Downloads/dion/d2.jpeg'\n",
    "\n",
    "# Compare the images\n",
    "distance, is_same = compare_images(\n",
    "    model=inference_model,\n",
    "    image1_path=test_image1_path,\n",
    "    image2_path=test_image2_path,\n",
    "    transform=transform,\n",
    "    device=device,\n",
    "    threshold=0.65  # Adjust the threshold if necessary\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
